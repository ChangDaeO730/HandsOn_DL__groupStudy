{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def linear_layer(x):\n",
    "    return 3 * x + 2\n",
    "\n",
    "@tf.function\n",
    "def simple_nn(x):\n",
    "    return tf.nn.relu(linear_layer(x))\n",
    "\n",
    "def simple_function(x):\n",
    "    return 3 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.function`을 사용시에, 하나의 주 함수에만 어노테이션을 달면 거기에 호출된 다른 모든 함수는 자동으로 투명하게 최적화된 계산 그래프로 변환된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.def_function.Function at 0x25e20dd7dc8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn # 텐서플로 내부와 상호작용하는 특수 handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.simple_function(x)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_function # 일반 파이썬 handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_time :  0.09546189999991839\n",
      "auto_graph_time :  0.03926569999998719\n"
     ]
    }
   ],
   "source": [
    "# @tf.function 데코레이터 사용과 미사용 속도차이\n",
    "import timeit\n",
    "\n",
    "cell = tf.keras.layers.LSTMCell(100)\n",
    "\n",
    "@tf.function\n",
    "def fn(input, state):\n",
    "    return cell(input, state)\n",
    "\n",
    "input = tf.zeros([100, 100])\n",
    "state = [tf.zeros([100, 100])] * 2\n",
    "\n",
    "cell(input, state)\n",
    "fn(input, state)\n",
    "\n",
    "graph_time = timeit.timeit(lambda : cell(input, state), number = 100)\n",
    "auto_graph_time = timeit.timeit(lambda : fn(input, state), number = 100)\n",
    "print(\"graph_time : \", graph_time)\n",
    "print(\"auto_graph_time : \", auto_graph_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수형 api\n",
    "def build_model():\n",
    "    # 가변길이 정수 시퀀스\n",
    "    text_input_a = tf.keras.Input(shape = (None, ), dtype = \"int32\")\n",
    "    # 가변길이 정수 시퀀스\n",
    "    text_input_b = tf.keras.Input(shape = (None, ), dtype = \"int32\")\n",
    "    # 1000개의 고유 단어를 128차원 벡터에 매핑하여 임베딩\n",
    "    shared_embedding = tf.keras.layers.Embedding(1000, 128)\n",
    "    \n",
    "    # 두 입력을 인코딩하고자 동일 임베딩 계층 재사용\n",
    "    encoded_input_a = shared_embedding(text_input_a)\n",
    "    encoded_input_b = shared_embedding(text_input_b)\n",
    "    \n",
    "    # 2개의 로지스틱 예측\n",
    "    prediction_a = tf.keras.layers.Dense(1, activation = \"sigmoid\",\n",
    "                                        name = 'prediction_a')(encoded_input_a)\n",
    "    prediction_b = tf.keras.layers.Dense(1, activation = \"sigmoid\",\n",
    "                                        name = 'prediction_b')(encoded_input_b)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = [text_input_a, text_input_b],\n",
    "                           outputs = [prediction_a, prediction_b])\n",
    "    tf.keras.utils.plot_model(model, to_file = \"shared_model.png\")\n",
    "    \n",
    "build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# 서브클래싱 api\n",
    "class MyLayer(layers.Layer):\n",
    "    def __init(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name = \"kernel\",\n",
    "                                      shape = (input_shape[1], self.output_dim),\n",
    "                                      initializer = \"uniform\",\n",
    "                                      trainable = True)\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## useful Callbacks\n",
    "* tf.keras.callbacks.ModelCheckpoint\n",
    "* tf.keras.callbacks.LearningRateScheduler\n",
    "* tf.keras.callbacks.EarlyStopping\n",
    "* tf.keras.callbacks.TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델, 가중치 저장\n",
    "* model.save_weights(\"\")\n",
    "* model.load_weights(\"\")\n",
    "* model.save(\"name.h5\")\n",
    "* model = tf.keras.models.load_model(\"name.h5\")\n",
    "* json_string = model.to_json()\n",
    "* model = tf.keras.models.model_from_json(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow datasets\n",
    "* 원칙적인 방식으로 입력 데이터를 처리하는데 필요한 라이브러리\n",
    "---\n",
    "1. 생성 :\n",
    "  1. `from_tensor_slices()` : 개별(또는 다중) 넘파이배열(또는 텐서)를 받고 배치를 지원\n",
    "  2. `from tensors()` : 배치를 지원하지 않음\n",
    "  3. `from_generator()` : 제너레이터 함수에서 입력을 취함\n",
    "2. 변환 :\n",
    "  1. `batch()` : 순차적으로 데이터셋을 지정된 사이즈의 배치로 분할\n",
    "  2. `repeat()` : 데이터를 복제\n",
    "  3. `shuffle()` : 데이터 셔플\n",
    "  4. `map()` : 데이터에 함수 적용\n",
    "  5. `filter()` : 데이터를 거르고자 함수를 적용\n",
    "3. 반복자 :\n",
    "  1. `next_batch = iterator.get_next()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract_reasoning', 'accentdb', 'aeslc', 'aflw2k3d', 'ag_news_subset', 'ai2_arc', 'ai2_arc_with_ir', 'amazon_us_reviews', 'anli', 'arc', 'bair_robot_pushing_small', 'bccd', 'beans', 'big_patent', 'bigearthnet', 'billsum', 'binarized_mnist', 'binary_alpha_digits', 'blimp', 'bool_q', 'c4', 'caltech101', 'caltech_birds2010', 'caltech_birds2011', 'cars196', 'cassava', 'cats_vs_dogs', 'celeb_a', 'celeb_a_hq', 'cfq', 'cherry_blossoms', 'chexpert', 'cifar10', 'cifar100', 'cifar10_1', 'cifar10_corrupted', 'citrus_leaves', 'cityscapes', 'civil_comments', 'clevr', 'clic', 'clinc_oos', 'cmaterdb', 'cnn_dailymail', 'coco', 'coco_captions', 'coil100', 'colorectal_histology', 'colorectal_histology_large', 'common_voice', 'coqa', 'cos_e', 'cosmos_qa', 'covid19sum', 'crema_d', 'curated_breast_imaging_ddsm', 'cycle_gan', 'dart', 'davis', 'deep_weeds', 'definite_pronoun_resolution', 'dementiabank', 'diabetic_retinopathy_detection', 'div2k', 'dmlab', 'downsampled_imagenet', 'drop', 'dsprites', 'dtd', 'duke_ultrasound', 'e2e_cleaned', 'emnist', 'eraser_multi_rc', 'esnli', 'eurosat', 'fashion_mnist', 'flic', 'flores', 'food101', 'forest_fires', 'fuss', 'gap', 'geirhos_conflict_stimuli', 'genomics_ood', 'german_credit_numeric', 'gigaword', 'glue', 'goemotions', 'gpt3', 'groove', 'gtzan', 'gtzan_music_speech', 'hellaswag', 'higgs', 'horses_or_humans', 'howell', 'i_naturalist2017', 'imagenet2012', 'imagenet2012_corrupted', 'imagenet2012_real', 'imagenet2012_subset', 'imagenet_a', 'imagenet_r', 'imagenet_resized', 'imagenet_v2', 'imagenette', 'imagewang', 'imdb_reviews', 'irc_disentanglement', 'iris', 'kitti', 'kmnist', 'lambada', 'lfw', 'librispeech', 'librispeech_lm', 'libritts', 'ljspeech', 'lm1b', 'lost_and_found', 'lsun', 'lvis', 'malaria', 'math_dataset', 'mctaco', 'mlqa', 'mnist', 'mnist_corrupted', 'movie_lens', 'movie_rationales', 'movielens', 'moving_mnist', 'multi_news', 'multi_nli', 'multi_nli_mismatch', 'natural_questions', 'natural_questions_open', 'newsroom', 'nsynth', 'nyu_depth_v2', 'omniglot', 'open_images_challenge2019_detection', 'open_images_v4', 'openbookqa', 'opinion_abstracts', 'opinosis', 'opus', 'oxford_flowers102', 'oxford_iiit_pet', 'para_crawl', 'patch_camelyon', 'paws_wiki', 'paws_x_wiki', 'pet_finder', 'pg19', 'piqa', 'places365_small', 'plant_leaves', 'plant_village', 'plantae_k', 'qa4mre', 'qasc', 'quac', 'quickdraw_bitmap', 'race', 'radon', 'reddit', 'reddit_disentanglement', 'reddit_tifu', 'resisc45', 'robonet', 'rock_paper_scissors', 'rock_you', 's3o4d', 'salient_span_wikipedia', 'samsum', 'savee', 'scan', 'scene_parse150', 'scicite', 'scientific_papers', 'sentiment140', 'shapes3d', 'siscore', 'smallnorb', 'snli', 'so2sat', 'speech_commands', 'spoken_digit', 'squad', 'stanford_dogs', 'stanford_online_products', 'starcraft_video', 'stl10', 'story_cloze', 'sun397', 'super_glue', 'svhn_cropped', 'ted_hrlr_translate', 'ted_multi_translate', 'tedlium', 'tf_flowers', 'the300w_lp', 'tiny_shakespeare', 'titanic', 'trec', 'trivia_qa', 'tydi_qa', 'uc_merced', 'ucf101', 'vctk', 'vgg_face2', 'visual_domain_decathlon', 'voc', 'voxceleb', 'voxforge', 'waymo_open_dataset', 'web_nlg', 'web_questions', 'wider_face', 'wiki40b', 'wiki_bio', 'wiki_table_questions', 'wiki_table_text', 'wikihow', 'wikipedia', 'wikipedia_toxicity_subtypes', 'wine_quality', 'winogrande', 'wmt14_translate', 'wmt15_translate', 'wmt16_translate', 'wmt17_translate', 'wmt18_translate', 'wmt19_translate', 'wmt_t2t_translate', 'wmt_translate', 'wordnet', 'wsc273', 'xnli', 'xquad', 'xsum', 'xtreme_pawsx', 'xtreme_xnli', 'yelp_polarity_reviews', 'yes_no']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "builders = tfds.list_builders()\n",
    "print(builders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\user\\tensorflow_datasets\\rock_paper_scissors\\3.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9dc36c0475475f8f537414f543240b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65766bb50d2547df93e6dbb0c8c73b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling rock_paper_scissors-train.tfrecord...:   0%|          | 0/2520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling rock_paper_scissors-test.tfrecord...:   0%|          | 0/372 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset rock_paper_scissors downloaded and prepared to C:\\Users\\user\\tensorflow_datasets\\rock_paper_scissors\\3.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "tfds.core.DatasetInfo(\n",
      "    name='rock_paper_scissors',\n",
      "    full_name='rock_paper_scissors/3.0.0',\n",
      "    description=\"\"\"\n",
      "    Images of hands playing rock, paper, scissor game.\n",
      "    \"\"\",\n",
      "    homepage='http://laurencemoroney.com/rock-paper-scissors-dataset',\n",
      "    data_path='C:\\\\Users\\\\user\\\\tensorflow_datasets\\\\rock_paper_scissors\\\\3.0.0',\n",
      "    download_size=219.53 MiB,\n",
      "    dataset_size=219.23 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(300, 300, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=3),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=372, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=2520, num_shards=2>,\n",
      "    },\n",
      "    citation=\"\"\"@ONLINE {rps,\n",
      "    author = \"Laurence Moroney\",\n",
      "    title = \"Rock, Paper, Scissors Dataset\",\n",
      "    month = \"feb\",\n",
      "    year = \"2019\",\n",
      "    url = \"http://laurencemoroney.com/rock-paper-scissors-dataset\"\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data, info = tfds.load(\"rock_paper_scissors\", with_info = True)\n",
    "train_data, test_data = data['train'], data['test']\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_items = 100\n",
    "num_list = np.arange(num_items)\n",
    "\n",
    "num_list_dataset = tf.data.Dataset.from_tensor_slices(num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, info = tfds.load(\"imdb_reviews\", with_info = True, as_supervised = True)\n",
    "train_dataset = datasets['train']\n",
    "train_dataset = train_dataset.batch(5).shuffle(50).take(2)\n",
    "\n",
    "# for data in train_dataset:\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom gradient calcul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training = True)\n",
    "        regularization_loss = //\n",
    "        pred_loss = //\n",
    "        total_loss = pred_loss + regularization_loss\n",
    "    \n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for inputs, labels in train_data:\n",
    "        train_step(inputs, labels)\n",
    "    print(\"Finished epoch\", epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 효율적인 텐서플로 2.x 사용\n",
    "---\n",
    "1. 웬만하면 고수준 api 사용하기\n",
    "2. tf.function 데코레이터 추가하여 오토그래프로 그래프모드에서 효율적으로 수행되도록\n",
    "3. 파이썬 객체를 사용해 변수와 손실을 추적 -> tf.Variable 사용\n",
    "4. 데이터 입력으로는 tf.data 데이터셋을 사용하고 이 객체를 바로 tf.keras.Model.fit에 제공\n",
    "5. 가능하면 tf.layers 모듈을 사용해 순차형, 함수형 api로 사전정의된 블록 조합\n",
    "6. GPU, CPU, 여러 플랫폼으로의 분산전력 사용을 고려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
